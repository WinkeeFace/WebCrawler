
####### START HTTPS://DOCS.CLINE.BOT #######

Ask or search ... Ctrl + K Cline Documentation Getting Started Getting Started for New Coders Installing Dev Essentials Our Favorite Tech Stack Understanding Context Management Model Selection Guide Improving Your Prompting Skills Prompt Engineering Guide Custom Instructions Library Cline Memory Bank Exploring Cline's Tools Cline Tools Guide Checkpoints Plan & Act Modes: A Guide to Effective AI Development MCP Servers MCP Overview MCP Quickstart Building MCP Servers from GitHub Building Custom MCP Servers Custom Model Configs AWS Bedrock GCP Vertex AI LiteLLM & Cline (using Codestral) Running Models Locally Read Me First Ollama LM Studio Powered by GitBook On this page Getting Started Improving Your Prompting Skills Exploring Cline's Tools Contributing to Cline Additional Resources Cline Documentation Welcome to the Cline documentation - your comprehensive guide to using and extending Cline's capabilities. Here you'll find resources to help you get started, improve your skills, and contribute to the project. Getting Started New to coding? We've prepared a gentle introduction: Getting Started for New Coders Improving Your Prompting Skills Want to communicate more effectively with Cline? Explore: Prompt Engineering Guide Cline Memory Bank Exploring Cline's Tools Understand Cline's capabilities: Cline Tools Guide Extend Cline with MCP Servers: MCP Overview Building MCP Servers from GitHub Building Custom MCP Servers Contributing to Cline Interested in contributing? We welcome your input: Feel free to submit a pull request Contribution Guidelines Additional Resources Cline GitHub Repository: https://github.com/cline/cline MCP Documentation: https://modelcontextprotocol.org/docs We're always looking to improve this documentation. If you have suggestions or find areas that could be enhanced, please let us know. Your feedback helps make Cline better for everyone! Next Getting Started for New Coders Last updated 16 days ago

####### END HTTPS://DOCS.CLINE.BOT #######


####### START HTTPS://DOCS.CLINE.BOT/RUNNING-MODELS-LOCALLY/LM-STUDIO #######

Ask or search ... Ctrl + K Cline Documentation Getting Started Getting Started for New Coders Installing Dev Essentials Our Favorite Tech Stack Understanding Context Management Model Selection Guide Improving Your Prompting Skills Prompt Engineering Guide Custom Instructions Library Cline Memory Bank Exploring Cline's Tools Cline Tools Guide Checkpoints Plan & Act Modes: A Guide to Effective AI Development MCP Servers MCP Overview MCP Quickstart Building MCP Servers from GitHub Building Custom MCP Servers Custom Model Configs AWS Bedrock GCP Vertex AI LiteLLM & Cline (using Codestral) Running Models Locally Read Me First Ollama LM Studio Powered by GitBook On this page ü§ñ Setting Up LM Studio with Cline üìã Prerequisites üöÄ Setup Steps ‚ö†Ô∏è Important Notes üîß Troubleshooting Running Models Locally LM Studio A quick guide to setting up LM Studio for local AI model execution with Cline. Previous Ollama Last updated 22 days ago ü§ñ Setting Up LM Studio with Cline Run AI models locally using LM Studio with Cline. üìã Prerequisites Windows, macOS, or Linux computer with AVX2 support Cline installed in VS Code üöÄ Setup Steps 1. Install LM Studio Visit lmstudio.ai Download and install for your operating system 2. Launch LM Studio Open the installed application You'll see four tabs on the left: Chat , Developer (where you will start the server), My Models (where your downloaded models are stored), Discover (add new models) 3. Download a Model Browse the "Discover" page Select and download your preferred model Wait for download to complete 4. Start the Server Navigate to the "Developer" tab Toggle the server switch to "Running" Note: The server will run at http://localhost:1234 5. Configure Cline Open VS Code Click Cline settings icon Select "LM Studio" as API provider Select your model from the available options ‚ö†Ô∏è Important Notes Start LM Studio before using with Cline Keep LM Studio running in background First model download may take several minutes depending on size Models are stored locally after download üîß Troubleshooting If Cline can't connect to LM Studio: Verify LM Studio server is running (check Developer tab) Ensure a model is loaded Check your system meets hardware requirements

####### END HTTPS://DOCS.CLINE.BOT/RUNNING-MODELS-LOCALLY/LM-STUDIO #######


####### START HTTPS://DOCS.CLINE.BOT/RUNNING-MODELS-LOCALLY #######

Ask or search ... Ctrl + K Cline Documentation Getting Started Getting Started for New Coders Installing Dev Essentials Our Favorite Tech Stack Understanding Context Management Model Selection Guide Improving Your Prompting Skills Prompt Engineering Guide Custom Instructions Library Cline Memory Bank Exploring Cline's Tools Cline Tools Guide Checkpoints Plan & Act Modes: A Guide to Effective AI Development MCP Servers MCP Overview MCP Quickstart Building MCP Servers from GitHub Building Custom MCP Servers Custom Model Configs AWS Bedrock GCP Vertex AI LiteLLM & Cline (using Codestral) Running Models Locally Read Me First Ollama LM Studio Powered by GitBook

####### END HTTPS://DOCS.CLINE.BOT/RUNNING-MODELS-LOCALLY #######

